{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we create a neural network `Model` class:\n",
    "\n",
    "* Create a model by adding dense layers to it\n",
    "* Layers have relu activation function except the output layer which has sigmoid activation\n",
    "* Optimization algorithm would be gradient descent\n",
    "* Also have a train method to perform the training. The training will be initialise parameters, forward prop, compute cost, backward prop and update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(Z):\n",
    "    R = np.maximum(0, Z)\n",
    "    return R\n",
    "\n",
    "def sigmoid(Z):\n",
    "    S = 1 / (1 + np.exp(-1 * Z))\n",
    "    return S\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    Z[Z >= 0] = 1\n",
    "    Z[Z < 0]  = 0\n",
    "    return Z\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    SD = sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return SD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.L = 0\n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        self.A = {}\n",
    "        self.Z = {}\n",
    "        self.dA = {}\n",
    "        self.dZ = {}\n",
    "        self.dW = {}\n",
    "        self.db = {}\n",
    "        self.cost = 0.\n",
    "        self.m = 0\n",
    "        self.lam = 0\n",
    "        self.cost_history = []\n",
    "        self.acc_history = []\n",
    "        self.alpha_history = []\n",
    "        self.alpha = 0.\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def add_layers(self, list_of_layers):\n",
    "        self.layers = list_of_layers\n",
    "        self.L = len(self.layers) - 1 # Number of layers excluding the input feature layer\n",
    "    \n",
    "    def init_params(self):\n",
    "        for i in range(1, self.L + 1):\n",
    "            self.W[str(i)] = np.random.randn(self.layers[i], self.layers[i - 1]) * np.sqrt(2. / self.layers[i - 1])\n",
    "            self.b[str(i)] = np.zeros((self.layers[i], 1))\n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        self.A['0'] = X\n",
    "        \n",
    "        for i in range(1, self.L + 1):\n",
    "            self.Z[str(i)] = np.dot(self.W[str(i)], self.A[str(i - 1)]) + self.b[str(i)]\n",
    "            if i == self.L:\n",
    "                # Output layer, Sigmoid activation\n",
    "                self.A[str(i)] = sigmoid(self.Z[str(i)])\n",
    "            else:\n",
    "                # Hidden layer, Relu activataion\n",
    "                self.A[str(i)] = relu(self.Z[str(i)])\n",
    "    \n",
    "    def compute_cost(self, Y):\n",
    "        self.cost = -1 * np.sum(np.multiply(Y, np.log(self.A[str(self.L)])) + \n",
    "                           np.multiply(1 - Y, np.log(1 - self.A[str(self.L)]))) / self.m \n",
    "        \n",
    "        if self.lam != 0:\n",
    "            reg = (self.lam / (2 * self.m))\n",
    "            for i in range(1, self.L + 1):\n",
    "                reg += np.sum(np.dot(self.W[str(i)], self.W[str(i)].T))\n",
    "            self.cost += reg\n",
    "            \n",
    "        self.cost_history.append(self.cost)\n",
    "    \n",
    "    def backward_prop(self, Y):\n",
    "        # We need dA[str(L)] to start the backward prop computation\n",
    "        self.dA[str(self.L)] = -1 * (np.divide(Y, self.A[str(self.L)]) - np.divide(1 - Y, 1 - self.A[str(self.L)]))\n",
    "        self.dZ[str(self.L)] = np.multiply(self.dA[str(self.L)], sigmoid_derivative(self.Z[str(self.L)]))\n",
    "        self.dW[str(self.L)] = np.dot(self.dZ[str(self.L)], self.A[str(self.L - 1)].T) / self.m + (self.lam/self.m) * self.W[str(self.L)]\n",
    "        self.db[str(self.L)] = np.sum(self.dZ[str(self.L)], axis = 1, keepdims = True) / self.m\n",
    "        self.dA[str(self.L - 1)] = np.dot(self.W[str(self.L)].T, self.dZ[str(self.L)])\n",
    "            \n",
    "        for i in reversed(range(1, self.L)):\n",
    "\n",
    "            self.dZ[str(i)] = np.multiply(self.dA[str(i)], relu_derivative(self.Z[str(i)]))\n",
    "            self.dW[str(i)] = np.dot(self.dZ[str(i)], self.A[str(i - 1)].T) / self.m + (self.lam/self.m) * self.W[str(i)]\n",
    "            self.db[str(i)] = np.sum(self.dZ[str(i)], axis = 1, keepdims = True) / self.m\n",
    "            self.dA[str(i - 1)] = np.dot(self.W[str(i)].T, self.dZ[str(i)])\n",
    "    \n",
    "    def update_params(self):\n",
    "        for i in range(1, self.L + 1):\n",
    "            self.W[str(i)] = self.W[str(i)] - self.alpha * self.dW[str(i)]\n",
    "            self.b[str(i)] = self.b[str(i)] - self.alpha * self.db[str(i)]\n",
    "    \n",
    "    def train(self, X, Y, iterations = 10, \n",
    "        alpha = 0.001, decay = True, decay_iter = 5, decay_rate = 0.9, stop_decay_counter = 100,\n",
    "        verbose = True, lam = 0):\n",
    "        \n",
    "        self.m = Y.shape[1]\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.lam = lam\n",
    "        \n",
    "        # initialize parameters\n",
    "        self.init_params()\n",
    "\n",
    "        for i in range(iterations):\n",
    "            # forward prop\n",
    "            self.forward_prop(X)\n",
    "            # compute cost\n",
    "            self.compute_cost(Y)\n",
    "            # backward prop\n",
    "            self.backward_prop(Y)\n",
    "            # update params\n",
    "            self.update_params()\n",
    "            # evaluate\n",
    "            self.acc_history.append(self.evaluate(X, Y, in_training = True))\n",
    "            # save alpha\n",
    "            self.alpha_history.append(self.alpha)\n",
    "            # learning rate decay\n",
    "            if decay and stop_decay_counter > 0 and i % decay_iter == 0:\n",
    "                self.alpha = decay_rate * self.alpha\n",
    "                stop_decay_counter -= 1\n",
    "            # display cost per iteration\n",
    "            if verbose:\n",
    "                print('Cost after {} iterations: {}'.format(i, self.cost))\n",
    "    \n",
    "    def predict(self, X, in_training = False):\n",
    "        if in_training == False:\n",
    "            self.forward_prop(X)\n",
    "            \n",
    "        preds = self.A[str(self.L)] >= 0.5\n",
    "        preds = np.squeeze(preds)\n",
    "        return preds\n",
    "        \n",
    "    def evaluate(self, X, Y, in_training = False):\n",
    "        examples = X.shape[1]\n",
    "        \n",
    "        pred = self.predict(X, in_training = in_training)\n",
    "        pred = pred.reshape(1, examples)\n",
    "        diff = np.sum(abs(pred - Y))\n",
    "        acc = (examples - np.sum(diff)) / examples\n",
    "        return acc\n",
    "    \n",
    "    def plot_cost(self):\n",
    "        plt.plot(range(self.iterations), self.cost_history, color = 'b')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_acc(self):\n",
    "        plt.plot(range(self.iterations), self.acc_history, color = 'r')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_alpha(self):\n",
    "        plt.plot(range(self.iterations), self.alpha_history, color = 'g')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model()\n",
    "m.add_layers([2, 2, 1])\n",
    "X = np.array([\n",
    "    [1., 0., 0., 0., -10., 9.],\n",
    "    [0., 1., 0., -6., -4., -1.]\n",
    "])\n",
    "Y = np.array([1, 1, 1, 0, 0, 1])\n",
    "Y = Y.reshape(1, 6)\n",
    "\n",
    "m.train(X, Y, iterations = 50, alpha = 0.1, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_cost()\n",
    "m.plot_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.evaluate(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haberman Survival Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains cases from study conducted on the survival of patients who had undergone surgery for breast cancer. Get the data and its description from UCI's [Machine Learning Repo](https://archive.ics.uci.edu/ml/datasets/Haberman%27s+Survival)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data', sep = ',', header = None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three features and one class. 1 means that the patient survived for 5 years or longer and 2 means that the patient died within 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.iloc[:,:-1]\n",
    "Y_train = data.iloc[:, -1]\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(X_train, axis = 0)\n",
    "variance = np.var(X_train, axis = 0)\n",
    "\n",
    "X_train = np.divide((X_train - mean), variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train - 1\n",
    "# Changing label 1 to 0 and label 2 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and train sets\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "X_test = X_train[250:,:]\n",
    "Y_test = Y_train[250:,:]\n",
    "\n",
    "X_train_ = X_train[:250,:]\n",
    "Y_train_ = Y_train[:250,:]\n",
    "\n",
    "print(X_train_.shape)\n",
    "print(Y_train_.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train_.reshape(3, 250)\n",
    "Y_train_ = Y_train_.reshape(1, 250)\n",
    "X_test  = X_test.reshape(3, 56)\n",
    "Y_test  = Y_test.reshape(1, 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model()\n",
    "m.add_layers([3, 16, 16, 1])\n",
    "\n",
    "m.train(X_train_, Y_train_, alpha = 0.9, decay_rate = 0.98, decay_iter = 10, stop_decay_counter = 100, \n",
    "        iterations = 5000, verbose = False, lam = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test set acc = ', m.evaluate(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Similar Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train_k = X_train_.reshape(250, 3)\n",
    "Y_train_k = Y_train_.reshape(250, 1)\n",
    "X_test_k  = X_test.reshape(56, 3)\n",
    "Y_test_k  = Y_test.reshape(56, 1)\n",
    "Y_train_k = to_categorical(Y_train_k)\n",
    "Y_test_k = to_categorical(Y_test_k)\n",
    "\n",
    "m_k = Sequential([\n",
    "    Dense(16, activation = 'relu', input_shape = (3,)),\n",
    "    Dense(16, activation = 'relu'),\n",
    "    Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    "m_k.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = 'sgd',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "m_k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = m_k.fit(X_train_k, Y_train_k, epochs = 100, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(100), history.history['loss']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = m_k.evaluate(X_test_k, Y_test_k)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
