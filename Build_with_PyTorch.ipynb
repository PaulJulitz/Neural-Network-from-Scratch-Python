{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7bca0e",
   "metadata": {},
   "source": [
    "# Part 1: Linear Transformation Input - 1 Hidden Layer\n",
    "Given an input tensor $x$ of shape $(100, 3)$, a linear layer (fully connected layer) is applied:\n",
    "\n",
    "$$\n",
    "\\vec{y}_1 = \\vec{x}W^T_1 + \\vec{b}_1\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\vec{x} $ is the input tensor,\n",
    "- $ W_1 $ is the weight matrix for the hidden layer one of shape $(4, 3)$ (since the layer maps from 3 input features to 4 output features (neurons)),\n",
    "- $ b_1 $ is the bias vector of shape $(4)$. For each neuron 1 $b_{1,j}$ value.\n",
    "- $ \\vec{y}_1 $ is the output tensor of shape $(100, 4)$.\n",
    "\n",
    "This operation is performed by the `nn.Linear` module in PyTorch, resulting in a new tensor where each row is a linear transformation of the corresponding input row.\n",
    "\n",
    "<center>\n",
    "\n",
    "![Description](images/Scc001.png)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a725535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Create a dataset with 3 featrures (x_input_dataset_1, x_input_dataset_2, x_input_dataset_3) and 1000 samples.\n",
    "# Each sample is a vector of 3 features.\n",
    "x_input_dataset_input_dataset = torch.randn(100,3) # create a input dataset\n",
    "\n",
    "# The nn.Linear layer performs two main tasks:\n",
    "# 1. Initializes the weight matrix_input_dataset $W$ and bias vector $b$.\n",
    "# 2. Applies the affine transformation $\\vec{y} = \\vec{x_input_dataset} W^T + \\vec{b}$ to the input data $\\vec{x_input_dataset}$.\n",
    "#    Here, $\\vec{x_input_dataset}$ is the input tensor, $W$ is the weight matrix_input_dataset, and $b$ is the bias vector.\n",
    "hidden_hidden_layer_1 = nn.Linear(\n",
    "    in_features=3,  # Number of input features ($x_input_dataset_1$, $x_input_dataset_2$, $x_input_dataset_3$), not the dataset size.\n",
    "    out_features=4  # Number of output features ($y_1$, $y_2$, $y_3$, $y_4$), i.e., neurons in this layer.\n",
    ")\n",
    "\n",
    "# Pass the input tensor $\\vec{x_input_dataset}$ (1000 datasets by 3 features) through the linear layer.\n",
    "# This computes the output $\\vec{y} = \\vec{x_input_dataset} W^T + \\vec{b}$,\n",
    "y_1 = hidden_hidden_layer_1(x_input_dataset_input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50ce377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initalised weight matrix_input_dataset W:\n",
      "\n",
      "Input tensor x_input_dataset shape: torch.Size([100, 3])\n",
      "Weight matrix W_1 shape: torch.Size([4, 3])\n",
      "Bias vector b_1 shape: torch.Size([4])\n",
      "Output tensor y_1 shape: torch.Size([100, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Initalised weight matrix_input_dataset W:\\n\")\n",
    "print(\"Input tensor x_input_dataset shape:\", x_input_dataset.shape)         # (5, 3): 5 samples, each with 3 features\n",
    "print(\"Weight matrix W_1 shape:\", hidden_layer_1.weight.shape) # (4, 3): 4 output neurons, each with 3 weights (one per input feature)\n",
    "print(\"Bias vector b_1 shape:\", hidden_layer_1.bias.shape)     # (4,): 4 output neurons, each with a bias\n",
    "print(\"Output tensor y_1 shape:\", y_1.shape)        # (5, 4): 5 samples, each with 4 outputs (one per neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53035def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn.functional as F\n",
    "\t\n",
    "\t\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_feature_size, layer_one_nodes, layer_two_nodes, output_category):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_feature_size, layer_one_nodes)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weights, nonlinearity=\"relu\")\n",
    "        self.fc2 = nn.Linear(layer_one_nodes,layer_two_nodes)\n",
    "        self.fc3 = nn.Linear(layer_two_nodes, output_category)\n",
    "\n",
    "    # Apply activation function to first layer \n",
    "    def forward(self, x_input_dataset):\n",
    "        x_input_dataset = F.relu(self.fc1(x_input_dataset))\n",
    "        x_input_dataset = F.relu(self.fc2(x_input_dataset))\n",
    "        x_input_dataset = F.sigmoid(self.fc3(x_input_dataset))\n",
    "        \n",
    "        return x_input_dataset\n",
    "\n",
    "# Create an instance of the network\n",
    "ds_feature_size = 4\n",
    "hidden_layer_one_size = 5\n",
    "hidden_layer_two_size = 3\n",
    "ds_target_size = 5\n",
    "\n",
    "# Create instance of own neural network\n",
    "NN_1 = FFNN(ds_feature_size, hiddenlayer_one_size, hidden_layer_two_size, ds_target_size)\n",
    "\n",
    "print(\"Architecture of instance of the fully connected neural network: \", NN_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSci_NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
